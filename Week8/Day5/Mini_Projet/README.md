# ğŸ“š MÃ©ta-analyse de travaux rÃ©cents sur les LLMs

Ce dÃ©pÃ´t contient un rapport dâ€™analyse croisÃ©e de 4 articles rÃ©cents sur les modÃ¨les de langage de grande taille (LLMs), avec un focus sur lâ€™**instruction tuning** et les stratÃ©gies de sÃ©lection ou dâ€™optimisation des donnÃ©es.

## ğŸ“„ Articles analysÃ©s

1. **Li et al. (2024)** â€“ *Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning*  
   ğŸ”— https://arxiv.org/abs/2402.10110

2. **Wei et al. (2024)** â€“ *Instruction-tuned Language Models are Better Knowledge Learners*  
   ğŸ”— https://aclanthology.org/2024.acl-long.296

3. **Li et al. (2024)** â€“ *Understanding the Behavior Shift in LLMs after Instruction Tuning*  
   ğŸ”— https://aclanthology.org/2024.naacl-long.130

4. **Zheng et al. (2024)** â€“ *Automatic Data Optimization with LLM Agents for Instruction Tuning* (NeurIPS 2024)  
   ğŸ”— https://proceedings.neurips.cc/paper_files/paper/2024/hash/0852b88e96d973bd4e21b673f51621d0-Abstract-Conference.html
