{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1 : Install llama-cpp-python with CUDA bindings and clone the llama.cpp GitHub repository**"
      ],
      "metadata": {
        "id": "YSB9mQDe1yk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEjHDoTSxO2A"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python --verbose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "Bg5n-EPz2A2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 : Model Preparation : Convert the Hugging Face Code Llama model into GGUF format (optimized for llama.cpp)**"
      ],
      "metadata": {
        "id": "rTBo_bYa5eDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf"
      ],
      "metadata": {
        "id": "v4o4xc604TQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 : Load the converted model using the llama-cpp-python bindings**"
      ],
      "metadata": {
        "id": "gBLD-wzh-PrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Initialize the Python environment:**"
      ],
      "metadata": {
        "id": "-cAfHkq_-xSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "OL5hfkkt-QBN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load the GGUF model:**"
      ],
      "metadata": {
        "id": "bAw2CIRj-z8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "ADuKyqpt_n8P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "    model_path=\"/content/llama-2-7b-chat.Q4_K_M.gguf\",\n",
        "    n_gpu_layers=35,\n",
        "    n_threads=8\n",
        ")"
      ],
      "metadata": {
        "id": "WkgHLOqP-ZgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Generate text using a natural language prompt (e.g., ask about the solar system)**"
      ],
      "metadata": {
        "id": "Jwb9qpbN_ws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain how the solar system formed.\"\n",
        "output = llm(prompt=prompt, max_tokens=200)\n",
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3nxQl9a_qoV",
        "outputId": "078d4a9e-10a0-4460-c33d-4ebba243466e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 8 prefix-match hit, remaining 1 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     613.53 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   33085.66 ms /   200 runs   (  165.43 ms per token,     6.04 tokens per second)\n",
            "llama_perf_context_print:       total time =   33234.47 ms /   201 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hinweis: You can use the textbook chapter 8 for reference.\n",
            "\n",
            "The formation of the solar system is a complex and still somewhat mysterious process, but scientists have pieced together a general outline of how it all happened. Here are the key steps:\n",
            "1. The formation of the Sun: About 4.6 billion years ago, a giant cloud of gas and dust collapsed under its own gravity, forming the Sun. This cloud was likely made up of hydrogen and helium, with traces of heavier elements.\n",
            "2. The protoplanetary disk: As the Sun formed, the remaining gas and dust in the cloud began to rotate around it, forming a rotating disk called a protoplanetary disk. This disk was likely several times the size of the Sun and contained many small rocky bodies called planetesimals.\n",
            "3. Accretion and gravitational collapse: Over time, the planetesimals in the pro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Generate Python code by prompting the model to write a script (e.g., loading a model with Hugging Face)**"
      ],
      "metadata": {
        "id": "rjnM8N2-AdkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_stream = llm(\n",
        "   prompt=\"Write a Python script that loads a Hugging Face model and tokenizes input.\",\n",
        "   max_tokens=200,\n",
        "   stream=True\n",
        ")\n",
        "for token_info in output_stream:\n",
        "   print(token_info[\"choices\"][0][\"text\"], end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h3XiCjYAWBp",
        "outputId": "c90e8601-4fac-4d1e-cc4c-32115d76c985"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 17 prefix-match hit, remaining 1 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " script should also print the tokenized input and the predicted output for each input.\n",
            "\n",
            "Here is an example of a Hugging Face model:\n",
            "\n",
            "```\n",
            "from transformers import AutoModelForSequenceClassification, TrainingArguments\n",
            "\n",
            "# Load the model and tokenizer\n",
            "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
            "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
            "\n",
            "# Tokenize input text\n",
            "input_text = \"This is a sample input text.\"\n",
            "encoded_input = tokenizer.encode_plus(input_text, \n",
            "                                            max_length=512,\n",
            "                                            padding='max_length',\n",
            "                                            truncation=True)\n",
            "\n",
            "# Print the tokenized input and predicted output\n",
            "print(f\"Tokenized input: {encoded_input}\")\n",
            "predictions = model("
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     613.53 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =   31253.63 ms /   200 runs   (  156.27 ms per token,     6.40 tokens per second)\n",
            "llama_perf_context_print:       total time =   31675.24 ms /   201 tokens\n"
          ]
        }
      ]
    }
  ]
}