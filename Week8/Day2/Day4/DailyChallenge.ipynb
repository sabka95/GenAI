{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Taxonomie des d√©fis (hi√©rarchie des cat√©gories)***\n",
        "## **1. Comprendre les LLM**\n",
        "\n",
        "Compr√©hension du fonctionnement interne\n",
        "\n",
        "Interpr√©tation des r√©ponses du mod√®le\n",
        "\n",
        "Limites et comportements impr√©vus\n",
        "\n",
        "## **2. Construction de requ√™tes (prompting)**\n",
        "\n",
        "Techniques de prompt engineering\n",
        "\n",
        "Adaptation √† des t√¢ches sp√©cifiques\n",
        "\n",
        "Prompt chaining et structuration\n",
        "\n",
        "## **3. √âvaluation**\n",
        "\n",
        "√âvaluation des performances (subjective vs objective)\n",
        "\n",
        "Outils et m√©triques\n",
        "\n",
        "Benchmarks manquants ou inad√©quats\n",
        "\n",
        "## **4. Donn√©es**\n",
        "\n",
        "Collecte et nettoyage de donn√©es\n",
        "\n",
        "Alignement des donn√©es d'entr√©e/sortie\n",
        "\n",
        "√âtiquetage et annotation\n",
        "\n",
        "## **5. D√©ploiement**\n",
        "\n",
        "Int√©gration dans des pipelines/applications\n",
        "\n",
        "Scalabilit√©\n",
        "\n",
        "Co√ªt et latence\n",
        "\n",
        "## **6. S√©curit√© et √©thique**\n",
        "\n",
        "Contenu toxique\n",
        "\n",
        "Fuites d‚Äôinformations sensibles\n",
        "\n",
        "Biais algorithmiques\n",
        "\n",
        "## **7. Outils et plateformes**\n",
        "\n",
        "Limitations des interfaces API\n",
        "\n",
        "Documentation insuffisante\n",
        "\n",
        "Manque d‚Äôexemples ou SDK"
      ],
      "metadata": {
        "id": "gsP1vyY-QM0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Quelles sont les d√©cisions cl√©s de conception prises dans leur m√©thodologie empirique ?\n",
        "\n",
        "- **Source des donn√©es** : Les auteurs ont extrait 1 114 posts sur Stack Overflow et 1 224 issues GitHub li√©s aux LLMs (comme OpenAI, Hugging Face, etc.).\n",
        "- **M√©thodologie hybride** : Une approche inductive (grounded theory) a √©t√© utilis√©e pour construire une taxonomie des probl√®mes rencontr√©s.\n",
        "- **S√©lection guid√©e par l‚Äôusage** : Les auteurs ont filtr√© les posts selon des crit√®res comme la popularit√© (votes, commentaires), la clart√©, et le fait qu'ils illustrent des probl√®mes concrets.\n",
        "- **It√©ration** : La taxonomie a √©t√© raffin√©e au fil de plusieurs cycles de codage avec retour utilisateur (co-codage, discussion entre chercheurs).\n",
        "\n",
        "### 2. Comment les auteurs ont-ils assur√© la validit√© et la fiabilit√© de leur proc√©dure de codage ?\n",
        "\n",
        "- **Codage ind√©pendant par deux chercheurs** : Chaque post a √©t√© cod√© ind√©pendamment pour r√©duire le biais individuel.\n",
        "- **Mesure d‚Äôaccord inter-annotateurs** : Le kappa de Cohen a √©t√© utilis√© pour quantifier la fiabilit√© du codage.\n",
        "- **R√©solution collaborative des conflits** : Les d√©saccords ont √©t√© r√©solus par discussion jusqu‚Äô√† consensus.\n",
        "- **Validation externe** : La taxonomie finale a √©t√© revue avec d'autres chercheurs et experts.\n",
        "\n",
        "### 3. Quels types de d√©fis dominent le d√©veloppement des LLMs, selon les donn√©es ?\n",
        "\n",
        "- **Prompt Engineering** : C‚Äôest l‚Äôun des d√©fis les plus fr√©quemment discut√©s, notamment la formulation efficace de prompts.\n",
        "- **Compr√©hension des mod√®les** : Les d√©veloppeurs sont souvent perdus face aux r√©ponses impr√©visibles des mod√®les.\n",
        "- **√âvaluation** : Manque de m√©thodes objectives et reproductibles pour √©valuer les performances des mod√®les.\n",
        "- **Limitations des plateformes** : Probl√®mes li√©s aux API (taux limites, manque de documentation, etc.).\n",
        "\n",
        "### 4. Quelles implications ces d√©fis ont-ils pour la conception des plateformes ou API LLM ?\n",
        "\n",
        "- **Besoin de meilleures interfaces** : Des outils de d√©bogage ou de visualisation des r√©ponses seraient utiles.\n",
        "- **Standardisation des √©valuations** : Fournir des benchmarks int√©gr√©s dans les plateformes pour faciliter la validation.\n",
        "- **Documentation + cas d‚Äôusage** : N√©cessit√© de guides plus riches, y compris pour des t√¢ches complexes.\n",
        "- **Personnalisation du comportement des mod√®les** : Les API devraient permettre plus de contr√¥le sur la sortie g√©n√©r√©e.\n",
        "\n",
        "### 5. Propositions originales de ressources/outils communautaires\n",
        "\n",
        "#### üìå Outil 1 : PromptLab ‚Äî Un environnement interactif de test de prompts\n",
        "\n",
        "- Interface low-code/no-code pour tester, versionner et comparer diff√©rents prompts.\n",
        "- Int√©gration avec des m√©triques de qualit√© automatis√©es (comme perplexit√© ou diversit√©).\n",
        "- Syst√®me de partage communautaire avec notation des prompts efficaces.\n",
        "\n",
        "#### üß† Outil 2 : LLM Error Explainer ‚Äî D√©bogueur intelligent de r√©ponses\n",
        "\n",
        "- Outil qui analyse une r√©ponse LLM \"inattendue\" et propose des raisons possibles (ex. : ambigu√Øt√© du prompt, longueur de contexte‚Ä¶).\n",
        "- Suggestions automatiques de reformulation du prompt.\n",
        "- Historique visuel des essais pour voir l‚Äô√©volution des performances.\n",
        "\n"
      ],
      "metadata": {
        "id": "0kBKQ5J9Q7VO"
      }
    }
  ]
}