{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Forcer le couple compatible avec 0.8.5\n",
        "!pip install -Uq \"google-generativeai==0.8.5\" \"google-ai-generativelanguage==0.6.15\" \"langchain-google-genai==2.0.5\"\n",
        "\n",
        "# (Optionnel) Si un autre paquet re-tire une mauvaise version :\n",
        "!pip install -Uq --no-deps \"google-generativeai==0.8.5\" \"google-ai-generativelanguage==0.6.15\"\n"
      ],
      "metadata": {
        "id": "wI9LV0lPQMzd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Kf_DaR8pGX9U"
      },
      "outputs": [],
      "source": [
        "# --- Imports & types ---\n",
        "import os\n",
        "from typing import Annotated, Sequence, TypedDict, List, Literal\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain core\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Vector store / loaders\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "# LLM: Google Gemini seulement\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Pydantic\n",
        "from pydantic import BaseModel, Field\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- AgentState & grading model ---\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    documents: List[Document]\n",
        "    loops: int\n",
        "\n",
        "class grade(BaseModel):\n",
        "    # garde le nom et la signature que tu avais\n",
        "    binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "MAX_LOOPS = 2\n"
      ],
      "metadata": {
        "id": "ihkqP6eEPhT9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PATCH 1: LLM: flash ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def make_llm():\n",
        "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY est absent.\")\n",
        "    # üëâ mod√®le plus tol√©rant au quota\n",
        "    return ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n"
      ],
      "metadata": {
        "id": "97sWIZN2QcV7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PATCH 2: Tokens ---\n",
        "def build_vectorstore(urls: list, persist_dir: str = \".chroma\"):\n",
        "    import os\n",
        "    from langchain_community.document_loaders import WebBaseLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    try:\n",
        "        vs = Chroma(persist_directory=persist_dir, embedding_function=emb)\n",
        "        _ = vs._collection.count()\n",
        "        return vs\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    docs = WebBaseLoader(urls).load()\n",
        "    # üëâ chunks plus courts\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    vs = Chroma.from_documents(chunks, embedding=emb, persist_directory=persist_dir)\n",
        "    vs.persist()\n",
        "    return vs\n",
        "\n",
        "def make_retriever_tool(vs):\n",
        "    # üëâ k=3 pour limiter le contexte\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
        "    from langchain.tools.retriever import create_retriever_tool\n",
        "    tool = create_retriever_tool(\n",
        "        retriever,\n",
        "        name=\"kb_search\",\n",
        "        description=\"Recherche dans la base Chroma.\"\n",
        "    )\n",
        "    return tool, retriever\n"
      ],
      "metadata": {
        "id": "zhhqFw5JQdpA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Nodes: My_AI_Assistant, Vector_Retriever, Query_Rewriter, Output_Generator ---\n",
        "def make_nodes(llm, retriever_tool, retriever):\n",
        "    # Routeur\n",
        "    router_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"D√©cide si la question a besoin de la base (appel kb_search) ou r√©ponds directement.\"),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ])\n",
        "    llm_router = llm.bind_tools([retriever_tool])\n",
        "\n",
        "    def ai_assistant(state: AgentState):\n",
        "        resp = (router_prompt | llm_router).invoke({\"messages\": state[\"messages\"]})\n",
        "        return {\"messages\": [resp]}\n",
        "\n",
        "    # Retrieve\n",
        "    def retrieve(state: AgentState):\n",
        "        last_ai = next((m for m in reversed(state[\"messages\"]) if isinstance(m, AIMessage)), None)\n",
        "        if not last_ai or not getattr(last_ai, \"tool_calls\", None):\n",
        "            return {}\n",
        "        out_msgs: List[BaseMessage] = []\n",
        "        collected: List[Document] = []\n",
        "        for call in last_ai.tool_calls:\n",
        "            if call.get(\"name\") != \"kb_search\":\n",
        "                continue\n",
        "            query = call.get(\"args\", {}).get(\"query\")\n",
        "            if not query:\n",
        "                last_user = next((m for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), None)\n",
        "                query = last_user.content if last_user else \"\"\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            collected.extend(docs)\n",
        "            preview = []\n",
        "            for i, d in enumerate(docs, 1):\n",
        "                src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
        "                snippet = d.page_content[:350].replace(\"\\n\", \" \")\n",
        "                preview.append(f\"[{i}] {src}: {snippet}‚Ä¶\")\n",
        "            out_msgs.append(\n",
        "                ToolMessage(\n",
        "                    content=\"\\n\".join(preview) if preview else \"(Aucun document)\",\n",
        "                    name=\"kb_search\",\n",
        "                    tool_call_id=call.get(\"id\", \"0\"),\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": out_msgs, \"documents\": collected}\n",
        "\n",
        "    # Grader (structured output)\n",
        "    grader = llm.with_structured_output(grade)\n",
        "    judge_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Tu es un juge. R√©ponds STRICTEMENT {binary_score:'yes'|'no'} selon la pertinence globale.\"),\n",
        "        (\"human\", \"Question:\\n{q}\\n\\nDocs (extraits):\\n{ctx}\")\n",
        "    ])\n",
        "    def grade_documents(state: AgentState) -> Literal[\"Output_Generator\",\"Query_Rewriter\"]:\n",
        "        docs = state.get(\"documents\", [])\n",
        "        if not docs:\n",
        "            return \"Query_Rewriter\"\n",
        "        q = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
        "        ctx = \"\\n\\n\".join(d.page_content[:500] for d in docs)\n",
        "        res: grade = (judge_prompt | grader).invoke({\"q\": q, \"ctx\": ctx})\n",
        "        return \"Output_Generator\" if res.binary_score.strip().lower() == \"yes\" else \"Query_Rewriter\"\n",
        "\n",
        "    # Rewrite\n",
        "    rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"R√©√©cris la question pour am√©liorer la recherche (<= 25 mots). R√©ponse = requ√™te seule.\"),\n",
        "        (\"human\", \"Question:\\n{q}\")\n",
        "    ])\n",
        "    def rewrite(state: AgentState):\n",
        "        loops = state.get(\"loops\", 0)\n",
        "        if loops >= MAX_LOOPS:\n",
        "            return {\n",
        "                \"messages\": [AIMessage(content=\"Apr√®s plusieurs tentatives, je manque de sources. Peux-tu pr√©ciser la p√©riode, les entit√©s ou les mots-cl√©s ?\")],\n",
        "                \"documents\": [],\n",
        "                \"loops\": loops\n",
        "            }\n",
        "        q = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
        "        new_q = (rewrite_prompt | llm).invoke({\"q\": q}).content.strip()\n",
        "        return {\"messages\": [HumanMessage(content=new_q)], \"loops\": loops + 1, \"documents\": []}\n",
        "\n",
        "    # Generate\n",
        "    gen_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"R√©ponds en t'appuyant UNIQUEMENT sur les documents fournis. Cite bri√®vement les sources [n].\"),\n",
        "        (\"human\", \"Question:\\n{q}\\n\\nDocuments:\\n{ctx}\")\n",
        "    ])\n",
        "    def generate(state: AgentState):\n",
        "        docs = state.get(\"documents\", [])\n",
        "        if not docs:\n",
        "            return {\"messages\": [AIMessage(content=\"Je n'ai pas de documents pertinents √† citer. Peux-tu pr√©ciser ?\")]}\n",
        "        q = next((m.content for m in reversed(state[\"messages\"]) if isinstance(m, HumanMessage)), \"\")\n",
        "        ctx = \"\\n\\n\".join([f\"[{i}] {d.metadata.get('source','?')}\\n{d.page_content[:1200]}\"  # 1200 max\n",
        "                   for i, d in enumerate(docs, 1)])\n",
        "        resp = (gen_prompt | llm).invoke({\"q\": q, \"ctx\": ctx})\n",
        "        return {\"messages\": [AIMessage(content=resp.content)]}\n",
        "\n",
        "    return ai_assistant, retrieve, grade_documents, rewrite, generate\n"
      ],
      "metadata": {
        "id": "H4ZmryscQfBN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build workflow ---\n",
        "def should_call_tools(state: AgentState):\n",
        "    last_ai = next((m for m in reversed(state[\"messages\"]) if isinstance(m, AIMessage)), None)\n",
        "    return \"Vector_Retriever\" if (last_ai and getattr(last_ai, \"tool_calls\", None)) else \"END\"\n",
        "\n",
        "def build_app(urls: List[str]):\n",
        "    llm = make_llm()\n",
        "    vs = build_vectorstore(urls)\n",
        "    retriever_tool, retriever = make_retriever_tool(vs)\n",
        "    ai_assistant, retrieve, grade_documents, rewrite, generate = make_nodes(llm, retriever_tool, retriever)\n",
        "\n",
        "    workflow = StateGraph(AgentState)\n",
        "    workflow.add_node(\"My_AI_Assistant\", ai_assistant)\n",
        "    workflow.add_node(\"Vector_Retriever\", retrieve)\n",
        "    workflow.add_node(\"Query_Rewriter\", rewrite)\n",
        "    workflow.add_node(\"Output_Generator\", generate)\n",
        "\n",
        "    workflow.set_entry_point(\"My_AI_Assistant\")\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"My_AI_Assistant\",\n",
        "        should_call_tools,\n",
        "        {\"Vector_Retriever\": \"Vector_Retriever\", \"END\": END},\n",
        "    )\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"Vector_Retriever\",\n",
        "        grade_documents,\n",
        "        {\"Output_Generator\": \"Output_Generator\", \"Query_Rewriter\": \"Query_Rewriter\"},\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(\"Query_Rewriter\", \"My_AI_Assistant\")\n",
        "\n",
        "    return workflow.compile()\n"
      ],
      "metadata": {
        "id": "4eOA-N_tQgj7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test run (Gemini only) ---\n",
        "import os\n",
        "\n",
        "assert os.getenv(\"GOOGLE_API_KEY\"), \"GOOGLE_API_KEY manquant. Fournis ta cl√© avant de continuer.\"\n",
        "\n",
        "urls = [\n",
        "    \"https://python.langchain.com/docs/langgraph\",\n",
        "    \"https://python.langchain.com/docs/langgraph/concepts\",\n",
        "    \"https://python.langchain.com/docs/langgraph/how-tos\",\n",
        "    \"https://python.langchain.com/docs/langgraph/examples\",\n",
        "]\n",
        "\n",
        "app = build_app(urls)\n",
        "\n",
        "init_state = AgentState(messages=[HumanMessage(content=\"√Ä quoi sert StateGraph dans LangGraph ?\")],\n",
        "                        documents=[], loops=0)\n",
        "result = app.invoke(init_state)\n",
        "\n",
        "last = next((m for m in reversed(result[\"messages\"]) if isinstance(m, AIMessage)), None)\n",
        "print(\"=== R√©ponse ===\")\n",
        "print(last.content if last else \"(pas de r√©ponse)\")\n"
      ],
      "metadata": {
        "id": "4fHUTgUOQiuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}